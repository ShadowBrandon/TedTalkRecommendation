{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/rounakbanik/ted-talks#transcripts.csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "#For  New York -> New_York \n",
    "from nltk.tokenize  import MWETokenizer  # multi - word expression \n",
    "from nltk.tokenize  import word_tokenize\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.TedTalkdb\n",
    "transcripts_collection = db.transcripts #make a collection in our databased called new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(document): \n",
    "    #Return a cleaned string (or cleaned transcript)\n",
    "    '''\n",
    "    Function that goes through every document and does what is listed above\n",
    "    Also, need to clean the words within the document\n",
    "    '''\n",
    "    \n",
    "    #print(\"INSIDE:\")\n",
    "    #print(document)\n",
    "    mwe_tokenizer = MWETokenizer([( 'United' , 'States' ), ( 'New' ,  'York' ), ('High', 'School'), ('high', 'school'), ('New' ,'York','City'), ('New ','York', 'Times')])\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    stopword_list = stopwords.words()\n",
    "    stopword_list += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"','!', ';',\\\n",
    "             '.\\'\"','[',']','—',\".\\'\", 'ok','okay', 'felt', 'little','leave',' told ', 'sort',' told', 'yes','yeah','ya','stuff', ' 000 ',' em ','get','got',\\\n",
    "             ' oh ', 'oh',' oh', 'oh ','la','was','wa','?','like','go',' le ',' ca ',' I ',\" ? \",\"s\", ' t ','ve','guy', ' guy ', 're', 'every', 'single', 'old',\\\n",
    "            'year', 'ago', 'let', 'take' ] #told, went, came, able, example, hand, maybe, try, looking\n",
    "    document = re.sub(r'\\(.+?\\)', ' ', document)\n",
    "    document = re.sub(r'\\[.+?\\]', ' ', document)\n",
    "    document = re.sub(r'[^\\w\\s]',' ', document)\n",
    "    document = re.sub('\\w*\\d\\w*', ' ', document)\n",
    "    document = mwe_tokenizer.tokenize(word_tokenize(document))\n",
    "    document = ' '.join(document)\n",
    "    cleaned_words = []\n",
    "    for word in document.split():\n",
    "        low_word = lemmizer.lemmatize(word.lower())\n",
    "        #low_word = stemmer.stem(word.lower())\n",
    "        if low_word not in stopword_list:\n",
    "            cleaned_words.append(low_word)\n",
    "    return cleaned_words\n",
    "#keep guys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle the cleaned_document\n",
    "with open('cleaned_talks.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(cleaned_document, picklefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_preprocessor:\n",
    "    \n",
    "    def __init__(self, vectorizer=None, tokenizer=None, cleaning_function=None): #vectorizer=CountVectorizer()\n",
    "        if not tokenizer:\n",
    "            tokenizer = self.splitter\n",
    "        if not cleaning_function:\n",
    "            cleaning_function = self.clean_document#\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cleaning_function = cleaning_function#\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "        \n",
    "    def splitter(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits on spaces naively\n",
    "        \"\"\"\n",
    "        return text.split(' ')\n",
    "    \n",
    "    def fit(self, clean_text):\n",
    "        \"\"\"\n",
    "        Cleans the data and then fits the vectorizer with\n",
    "        the user provided text\n",
    "        \"\"\"\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "        \n",
    "    def transform(self, clean_text):\n",
    "        \"\"\"\n",
    "        Cleans any provided data and then transforms the data into\n",
    "        a vectorized format based on the fit function. Returns the\n",
    "        vectorized form of the data.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        #clean_text = self.cleaning_function(clean_text)#, self.tokenizer, self.stemmer) #pass an uncleaned version\n",
    "        #should this be another function that cleans EXCLUSIVELY WITH DATA THAT IS [\"\",\"\",...,\"\"]\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#(x['transcript'] for x in transcripts_collection.find())\n",
    "\n",
    "cursor = transcripts_collection.find()#transcripts_collection.aggregate([{'$sample':{'size': 2550}}])\n",
    "\n",
    "# This will go through every transcript and clean it through an instanciation of the class\n",
    "cleaned_document = []\n",
    "for document in cursor:\n",
    "    #print(document)\n",
    "    clean_text = clean_document(document['transcript'])\n",
    "    cleaned_document.append(' '.join(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will instanciate an object called nlp from the class nlp_preprocessor\n",
    "#THIS IS THE BEST ONE!\n",
    "nlp = nlp_preprocessor(CountVectorizer(\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1, 2), max_df = 0.4, min_df= 0, stop_words = 'english', max_features=3500), tokenizer=None, \n",
    "                 cleaning_function=clean_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_preprocessor(CountVectorizer(\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1, 2), max_df = 0.4, min_df= 0, stop_words = 'english', max_features=3000), tokenizer=None, \n",
    "                 cleaning_function=clean_document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_preprocessor(CountVectorizer(\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1, 2), max_df = 0.4, min_df=0 , stop_words = 'english', max_features=2500), tokenizer=None, \n",
    "                 cleaning_function=clean_document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "  (0, 0)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t4\n",
      "  (0, 12)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 85)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 105)\t3\n",
      "  (0, 154)\t4\n",
      "  (0, 158)\t3\n",
      "  (0, 164)\t2\n",
      "  (0, 188)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 204)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 241)\t1\n",
      "  (0, 245)\t2\n",
      "  (0, 252)\t1\n",
      "  (0, 296)\t1\n",
      "  :\t:\n",
      "  (2466, 2792)\t2\n",
      "  (2466, 2816)\t2\n",
      "  (2466, 2817)\t1\n",
      "  (2466, 2839)\t2\n",
      "  (2466, 2849)\t1\n",
      "  (2466, 2886)\t3\n",
      "  (2466, 2892)\t1\n",
      "  (2466, 2898)\t1\n",
      "  (2466, 2913)\t1\n",
      "  (2466, 3096)\t1\n",
      "  (2466, 3191)\t1\n",
      "  (2466, 3203)\t3\n",
      "  (2466, 3212)\t1\n",
      "  (2466, 3221)\t1\n",
      "  (2466, 3224)\t1\n",
      "  (2466, 3242)\t3\n",
      "  (2466, 3326)\t3\n",
      "  (2466, 3351)\t3\n",
      "  (2466, 3361)\t1\n",
      "  (2466, 3377)\t1\n",
      "  (2466, 3379)\t1\n",
      "  (2466, 3445)\t1\n",
      "  (2466, 3455)\t1\n",
      "  (2466, 3466)\t1\n",
      "  (2466, 3496)\t2\n",
      "      ability  able make  absolute  absolutely  abstract  abuse  academic  \\\n",
      "0           2          0         0           0         1      0         4   \n",
      "1           0          0         0           0         0      0         0   \n",
      "2           0          0         0           3         0      0         0   \n",
      "3           0          0         0           0         0      0         0   \n",
      "4           0          0         1           0         0      0         0   \n",
      "5           3          0         0           0         0      0         0   \n",
      "6           2          0         0           0         0      0         0   \n",
      "7           4          0         0           0         0      0         0   \n",
      "8           0          0         0           2         0      0         0   \n",
      "9           2          0         0           0         0      0         1   \n",
      "10          0          0         0           0         0      0         0   \n",
      "11          1          0         0           1         0      0         0   \n",
      "12          0          0         0           1         0      0         0   \n",
      "13          0          0         0           0         0      0         0   \n",
      "14          0          0         0           0         0      0         0   \n",
      "15          0          0         0           0         0      0         0   \n",
      "16          0          0         0           1         0      0         0   \n",
      "17          1          0         0           0         0      0         0   \n",
      "18          0          0         2           2         0      0         0   \n",
      "19          1          0         0           1         0      0         2   \n",
      "20          2          0         1           0         0      0         1   \n",
      "21          0          1         0           0         0      0         0   \n",
      "22          1          0         0           0         0      0         0   \n",
      "23          2          0         0           0         0      0         0   \n",
      "24          0          0         0           1         0      0         0   \n",
      "25          2          0         1           0         0      0         0   \n",
      "26          2          0         0           0         0      0         1   \n",
      "27          0          0         0           0         0      0         2   \n",
      "28          0          0         0           0         0      0         0   \n",
      "29          1          0         0           0         0      1         0   \n",
      "...       ...        ...       ...         ...       ...    ...       ...   \n",
      "2437        1          0         0           0         0      0         1   \n",
      "2438        1          0         0           0         0      0         0   \n",
      "2439        0          0         0           0         0      0         0   \n",
      "2440        1          0         0           0         0      0         0   \n",
      "2441        0          0         0           0         0      0         0   \n",
      "2442        0          0         0           0         0      0         0   \n",
      "2443        0          0         0           0         0      0         0   \n",
      "2444        3          0         0           0         0      3         0   \n",
      "2445        1          0         0           0         0      0         0   \n",
      "2446        0          0         0           0         0      0         0   \n",
      "2447        0          0         0           0         0      0         0   \n",
      "2448        0          0         0           0         0      0         0   \n",
      "2449        0          0         2           0         0      0         0   \n",
      "2450        0          0         0           0         0      0         0   \n",
      "2451        0          0         0           0         0      0         0   \n",
      "2452        0          0         0           1         0      0         0   \n",
      "2453        2          0         0           1         0      0         0   \n",
      "2454        0          0         0           0         0      0         0   \n",
      "2455        0          0         0           0         0      0         1   \n",
      "2456        0          0         0           0         0      0         0   \n",
      "2457        0          0         0           0         0      0         0   \n",
      "2458        0          0         0           0         0      0         0   \n",
      "2459        0          1         0           0         0      0         0   \n",
      "2460        0          1         0           0         0      0         0   \n",
      "2461        0          0         0           0         0      0         0   \n",
      "2462        0          0         0           0         0      0         0   \n",
      "2463        0          0         0           0         0      0         0   \n",
      "2464        1          0         0           1         1      0         0   \n",
      "2465        0          0         0           0         0      0         0   \n",
      "2466        0          0         0           0         0      0         0   \n",
      "\n",
      "      accept  accepted  access  ...   yield  young  young people  young woman  \\\n",
      "0          0         0       0  ...       0      0             0            0   \n",
      "1          0         0       0  ...       0      1             1            0   \n",
      "2          0         0       0  ...       0      0             0            0   \n",
      "3          0         0       0  ...       0      1             0            0   \n",
      "4          1         0       2  ...       0      0             0            0   \n",
      "5          0         0       0  ...       0      0             0            0   \n",
      "6          0         0       0  ...       0      1             0            0   \n",
      "7          1         0       0  ...       0      0             0            0   \n",
      "8          1         0       0  ...       0      0             0            0   \n",
      "9          0         0       0  ...       0      0             0            0   \n",
      "10         0         0       0  ...       0      2             0            1   \n",
      "11         0         0       2  ...       0      0             0            0   \n",
      "12         0         0       1  ...       0      2             0            0   \n",
      "13         0         0       0  ...       0      0             0            0   \n",
      "14         0         0       0  ...       0      0             0            0   \n",
      "15         0         0       0  ...       0      0             0            0   \n",
      "16         0         0       0  ...       0      0             0            0   \n",
      "17         0         0       0  ...       0      0             0            0   \n",
      "18         0         0       1  ...       0      0             0            0   \n",
      "19         0         0       1  ...       0      0             0            0   \n",
      "20         0         0       1  ...       0      0             0            0   \n",
      "21         0         0       0  ...       0      1             0            0   \n",
      "22         0         0       1  ...       0      0             0            0   \n",
      "23         0         1       0  ...       0      0             0            0   \n",
      "24         0         0       0  ...       0      0             0            0   \n",
      "25         0         0       0  ...       0      0             0            0   \n",
      "26         0         0       0  ...       0      0             0            0   \n",
      "27         0         0       2  ...       0      6             2            0   \n",
      "28         0         0       0  ...       0      0             0            0   \n",
      "29         1         0       0  ...       0      2             0            0   \n",
      "...      ...       ...     ...  ...     ...    ...           ...          ...   \n",
      "2437       0         0       0  ...       0      0             0            0   \n",
      "2438       0         0       0  ...       0      0             0            0   \n",
      "2439       0         0       0  ...       0      1             0            0   \n",
      "2440       0         0       1  ...       0      0             0            0   \n",
      "2441       0         0       0  ...       0      1             0            0   \n",
      "2442       0         0       0  ...       0      1             0            0   \n",
      "2443       1         0       1  ...       0      0             0            0   \n",
      "2444       0         0       1  ...       0      1             0            0   \n",
      "2445       0         0       0  ...       0      0             0            0   \n",
      "2446       0         0       0  ...       0      2             0            2   \n",
      "2447       1         0       0  ...       0      0             0            0   \n",
      "2448       0         0       0  ...       0      4             3            0   \n",
      "2449       0         0       0  ...       0      0             0            0   \n",
      "2450       0         0       0  ...       0      0             0            0   \n",
      "2451       0         0       1  ...       0      0             0            0   \n",
      "2452       0         0       0  ...       0      2             0            0   \n",
      "2453       1         0       0  ...       0      0             0            0   \n",
      "2454       0         0       0  ...       0      1             0            0   \n",
      "2455       0         0       0  ...       0      0             0            0   \n",
      "2456       0         0       0  ...       0      1             0            0   \n",
      "2457       0         0       1  ...       0      0             0            0   \n",
      "2458       0         0       3  ...       0      0             0            0   \n",
      "2459       0         0       0  ...       0      4             2            0   \n",
      "2460       0         0       0  ...       0      0             0            0   \n",
      "2461       1         0       1  ...       1      0             0            0   \n",
      "2462       0         0       0  ...       0      1             0            0   \n",
      "2463       0         0       1  ...       0      0             0            0   \n",
      "2464       0         0       0  ...       0      0             0            0   \n",
      "2465       0         0       0  ...       0      0             0            0   \n",
      "2466       0         0       0  ...       0      0             0            0   \n",
      "\n",
      "      younger  youth  youtube  zero  zone  zoom  \n",
      "0           0      0        0     0     0     0  \n",
      "1           0      0        0     0     0     0  \n",
      "2           0      0        0     0     0     0  \n",
      "3           0      0        0     0     0     0  \n",
      "4           0      0        0     0     0     0  \n",
      "5           0      0        0     1     0     0  \n",
      "6           0      0        0     0     0     0  \n",
      "7           0      0        0     0     2     0  \n",
      "8           0      0        0     1     0     0  \n",
      "9           0      0        0     0     0     0  \n",
      "10          0      1        0     0     0     0  \n",
      "11          0      0        0     0     0     0  \n",
      "12          0      0        0     1     1     0  \n",
      "13          0      0        0     0     0     2  \n",
      "14          0      0        0     2     0     0  \n",
      "15          0      0        0     0     0     0  \n",
      "16          0      0        0     0     0     0  \n",
      "17          0      0        0     1     0     0  \n",
      "18          0      0        0     0     0     0  \n",
      "19          0      0        0     0     0     0  \n",
      "20          0      0        0     0     0     0  \n",
      "21          0      0        0     0     0     0  \n",
      "22          0      0        0     0     0     0  \n",
      "23          0      0        0     0     0     0  \n",
      "24          3      0        0     0     0     0  \n",
      "25          0      0        0     1     0     0  \n",
      "26          0      0        0     2     0     0  \n",
      "27          0      0        0     0     1     0  \n",
      "28          0      0        0     0     0     0  \n",
      "29          0      0        0     0     0     0  \n",
      "...       ...    ...      ...   ...   ...   ...  \n",
      "2437        0      0        0     0     0     0  \n",
      "2438        0      0        0     0     0     0  \n",
      "2439        0      0        0     1     0     1  \n",
      "2440        0      0        0     0     0     0  \n",
      "2441        0      0        0     0     0     0  \n",
      "2442        0      0        0     0     0     0  \n",
      "2443        0      0        0     0     1     0  \n",
      "2444        0      0        0     0     0     0  \n",
      "2445        0      0        1     0     0     0  \n",
      "2446        0      0        0     1     0     0  \n",
      "2447        0      0        0     0     0     0  \n",
      "2448        1      3        0     0     0     0  \n",
      "2449        0      0        0     0     0     0  \n",
      "2450        0      0        0     0     0     0  \n",
      "2451        0      0        0     0     0     0  \n",
      "2452        0      1        0     0     0     0  \n",
      "2453        1      0        0     1     1     0  \n",
      "2454        1      0        0     0     1     0  \n",
      "2455        0      0        0     3     0     0  \n",
      "2456        0      0        0     0     0     0  \n",
      "2457        0      0        0     0     0     0  \n",
      "2458        0      0        0     0     0     0  \n",
      "2459        0      0        0     0     0     0  \n",
      "2460        0      0        0     0     0     0  \n",
      "2461        0      0        0     0     0     0  \n",
      "2462        1      0        0     0     0     0  \n",
      "2463        0      0        0     0     0     0  \n",
      "2464        0      0        0     0     0     0  \n",
      "2465        0      1        1     0     0     0  \n",
      "2466        0      0        2     0     0     0  \n",
      "\n",
      "[2467 rows x 3500 columns]\n"
     ]
    }
   ],
   "source": [
    "nlp.fit(cleaned_document)\n",
    "\n",
    "vectorized_docs_dense = nlp.transform(cleaned_document).toarray() #keep array if you want to construct data frame! or use the X\n",
    "print(vectorized_docs_dense)\n",
    "\n",
    "vectorized_docs = nlp.transform(cleaned_document) #keep array if you want to construct data frame! or use the X\n",
    "print(vectorized_docs)\n",
    "\n",
    "print(pd.DataFrame(vectorized_docs_dense, columns=nlp.vectorizer.get_feature_names())) #is it vectorized_docs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "\n",
    "n_comp = 17 #10 NO #8 NMF #3\n",
    "\n",
    "#lsa_tfidf = TruncatedSVD(n_components=n_comp)\n",
    "lsa_cv = TruncatedSVD(n_components=n_comp)\n",
    "nmf_cv = NMF(n_components=n_comp)\n",
    "\n",
    "#lsa_tfidf_data = lsa_tfidf.fit_transform(tfidf_data) #The computerized classification\n",
    "\n",
    "lsa_cv_data = lsa_cv.fit_transform(vectorized_docs) #The computerized classification\n",
    "nmf_cv_data = nmf_cv.fit_transform(vectorized_docs) #The computerized classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nlp.transform(cleaned_document) #Transform data through count vectorizer\n",
    "type(X) #<--- pass this to LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#ngrams (1,2), max_df = .4, min_df =0, max_feats = 3,500, n_topics=15, n_iters=70, rand = 42\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "n_topics = 15\n",
    "n_iter = 90\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                max_iter=n_iter,\n",
    "                                random_state=42,\n",
    "                               learning_method='online')\n",
    "\n",
    "data = lda.fit_transform(vectorized_docs) #nlp.transform(cleaned_document)) #lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_topics(lsa_cv,nlp.vectorizer.get_feature_names(),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_topics(nmf_cv,nlp.vectorizer.get_feature_names(),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "woman, love, girl, family, child, mother, friend, told, father, boy, night, young, knew, moment, saw, remember, later, asked, month, happened\n",
      "\n",
      "Topic  1\n",
      "dollar, money, africa, company, business, market, billion, economy, cost, global, economic, china, india, government, product, growth, job, term, poor, oil\n",
      "\n",
      "Topic  2\n",
      "brain, child, baby, study, neuron, social, data, behavior, sex, animal, area, health, memory, age, rate, difference, level, mental, sleep, activity\n",
      "\n",
      "Topic  3\n",
      "city, car, energy, water, air, foot, mile, hour, fly, power, space, road, street, wind, light, half, map, solar, bee, building\n",
      "\n",
      "Topic  4\n",
      "government, american, political, community, state, society, power, group, law, democracy, social, violence, city, black, public, police, nation, america, movement, issue\n",
      "\n",
      "Topic  5\n",
      "cell, cancer, patient, disease, body, drug, health, doctor, blood, gene, dna, medical, medicine, heart, treatment, virus, hospital, genome, genetic, surgery\n",
      "\n",
      "Topic  6\n",
      "technology, computer, robot, machine, device, video, phone, cell, camera, create, build, light, tool, lab, information, object, future, control, screen, digital\n",
      "\n",
      "Topic  7\n",
      "water, animal, ocean, specie, food, plant, earth, planet, fish, tree, forest, climate, ice, area, carbon, river, land, surface, coral, nature\n",
      "\n",
      "Topic  8\n",
      "data, information, internet, company, medium, online, network, google, web, technology, open, phone, computer, digital, algorithm, page, facebook, tool, website, social\n",
      "\n",
      "Topic  9\n",
      "language, book, read, english, chinese, ant, write, writing, ted, sentence, reading, written, card, letter, colony, china, learn, dictionary, speak, learning\n",
      "\n",
      "Topic  10\n",
      "self, experience, mind, choice, sense, future, reality, rule, god, wrong, value, science, love, nature, true, answer, culture, society, decision, happiness\n",
      "\n",
      "Topic  11\n",
      "kid, school, child, student, teacher, education, family, community, learn, job, learning, class, money, parent, teach, team, college, success, learned, university\n",
      "\n",
      "Topic  12\n",
      "design, building, art, project, space, image, artist, create, designer, museum, piece, architecture, material, wall, painting, form, object, house, built, city\n",
      "\n",
      "Topic  13\n",
      "play, game, music, sound, video, hear, playing, audience, piece, song, head, love, moment, eye, minute, pretty, movie, fun, interesting, mind\n",
      "\n",
      "Topic  14\n",
      "universe, planet, earth, space, light, science, star, physic, particle, galaxy, energy, theory, billion, sun, matter, image, pattern, picture, black, hole\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda,nlp.vectorizer.get_feature_names(),20) #with 3500  #n_topics = 15\n",
    "#n_iter = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE LDA AND COUNT VECTOR\n",
    "with open('vectorizer.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(nlp.vectorizer, picklefile)\n",
    "with open('lda.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(lda, picklefile)\n",
    "with open('lda_data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(data, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(data, axis=1)\n",
    "topic_ind.shape\n",
    "y=topic_ind\n",
    "\n",
    "# create text labels for plotting\n",
    "tsne_labels = pd.DataFrame(y)\n",
    "\n",
    "# save to csv\n",
    "tsne_labels.to_csv('tsne_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = tsne_labels\n",
    "topic_names[topic_names==0] = \"Family\"\n",
    "topic_names[topic_names==1] = \"Global Economy\"\n",
    "topic_names[topic_names==2] = \"Neurology\"\n",
    "topic_names[topic_names==3] = \"Transportation\"\n",
    "topic_names[topic_names==4] = \"Politics\"\n",
    "topic_names[topic_names==5] = \"Diseases\"\n",
    "topic_names[topic_names==6] = \"Techonology\"\n",
    "topic_names[topic_names==7] = \"Nature\"\n",
    "topic_names[topic_names==8] = \"Social Media\"\n",
    "topic_names[topic_names==9] = \"Language\"\n",
    "topic_names[topic_names==10] = \"Self-Help\"\n",
    "topic_names[topic_names==11] = \"Education\"\n",
    "topic_names[topic_names==12] = \"Architecture\"\n",
    "topic_names[topic_names==13] = \"Multimedia\"\n",
    "topic_names[topic_names==14] = \"Space\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Techonology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Multimedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Multimedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Social Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Social Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Multimedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>Diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>Self-Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>Space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>Diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>Global Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>Techonology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>Transportation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "0          Education\n",
       "1     Global Economy\n",
       "2       Social Media\n",
       "3           Politics\n",
       "4     Global Economy\n",
       "5          Self-Help\n",
       "6             Family\n",
       "7       Architecture\n",
       "8          Self-Help\n",
       "9          Self-Help\n",
       "10      Architecture\n",
       "11            Family\n",
       "12            Family\n",
       "13       Techonology\n",
       "14         Education\n",
       "15        Multimedia\n",
       "16        Multimedia\n",
       "17    Global Economy\n",
       "18      Architecture\n",
       "19      Social Media\n",
       "20      Social Media\n",
       "21        Multimedia\n",
       "22            Family\n",
       "23            Family\n",
       "24            Family\n",
       "25             Space\n",
       "26             Space\n",
       "27          Politics\n",
       "28         Self-Help\n",
       "29         Self-Help\n",
       "...              ...\n",
       "2437       Self-Help\n",
       "2438        Diseases\n",
       "2439          Family\n",
       "2440  Global Economy\n",
       "2441    Architecture\n",
       "2442       Education\n",
       "2443       Education\n",
       "2444        Politics\n",
       "2445    Architecture\n",
       "2446        Politics\n",
       "2447  Transportation\n",
       "2448        Politics\n",
       "2449       Self-Help\n",
       "2450  Transportation\n",
       "2451          Nature\n",
       "2452        Politics\n",
       "2453        Politics\n",
       "2454       Self-Help\n",
       "2455       Self-Help\n",
       "2456  Global Economy\n",
       "2457  Global Economy\n",
       "2458           Space\n",
       "2459          Family\n",
       "2460        Diseases\n",
       "2461  Global Economy\n",
       "2462        Politics\n",
       "2463          Nature\n",
       "2464     Techonology\n",
       "2465          Family\n",
       "2466  Transportation\n",
       "\n",
       "[2467 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save text labels to csv and pkl for plotting\n",
    "\n",
    "topic_names.to_csv('topic_names.csv')\n",
    "\n",
    "with open('topic_names.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(topic_names, picklefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best LDA - \n",
    "#### ngrams (1,2), max_df = .4, min_df =0, max_feats = 3,500, n_topics=15, n_iters=70, rand = 42\n",
    "Topic  0\n",
    "brain, patient, health, disease, cancer, doctor, drug, medical, body, treatment, hospital, heart, study, baby, child, blood, medicine, surgery, research, neuron\n",
    "\n",
    "Topic  1\n",
    "government, law, american, police, case, security, violence, state, military, prison, group, attack, united_states, weapon, soldier, crime, killed, conflict, gun, afghanistan\n",
    "\n",
    "Topic  2\n",
    "africa, india, global, china, government, economy, economic, child, african, growth, state, aid, poor, society, poverty, chinese, billion, population, community, family\n",
    "\n",
    "Topic  3\n",
    "computer, technology, game, machine, robot, video, play, sound, language, data, learning, student, brain, information, device, learn, music, algorithm, tool, pretty\n",
    "\n",
    "Topic  4\n",
    "water, ocean, fish, animal, coral, shark, boat, whale, plastic, specie, reef, mosquito, marine, dolphin, underwater, area, island, swim, deep, malaria\n",
    "\n",
    "Topic  5\n",
    "love, kid, friend, book, moment, told, night, child, mother, remember, week, later, family, head, minute, month, hour, room, knew, saw\n",
    "\n",
    "Topic  6\n",
    "data, internet, information, medium, online, network, social, phone, technology, web, open, book, google, facebook, page, government, friend, message, power, digital\n",
    "\n",
    "Topic  7\n",
    "design, building, art, project, space, create, image, artist, material, designer, piece, architecture, object, process, form, wall, museum, painting, built, light\n",
    "\n",
    "Topic  8\n",
    "science, brain, theory, rule, pattern, nature, model, physic, mind, reality, simple, scientist, force, law, consciousness, self, answer, sense, structure, line\n",
    "\n",
    "Topic  9\n",
    "planet, earth, light, universe, space, star, mar, galaxy, sun, billion, black, image, solar, dark, telescope, hole, energy, picture, fly, moon\n",
    "\n",
    "Topic  10\n",
    "city, car, energy, power, oil, street, building, air, fuel, technology, nuclear, road, mile, electricity, build, hour, solar, vehicle, built, half\n",
    "\n",
    "Topic  11\n",
    "child, school, kid, family, community, social, student, parent, self, experience, education, teacher, culture, young, society, group, black, american, love, learn\n",
    "\n",
    "Topic  12\n",
    "food, plant, climate, carbon, eat, farmer, water, climate change, bee, energy, global, vaccine, natural, grow, waste, billion, planet, feed, crop, solution\n",
    "\n",
    "Topic  13\n",
    "tree, water, foot, forest, earth, ice, animal, river, air, specie, cloud, mountain, bird, nature, planet, land, area, surface, body, picture\n",
    "\n",
    "Topic  14\n",
    "woman, girl, sex, boy, film, female, love, male, young, gender, sexual, baby, daughter, movie, marriage, partner, worker, village, age, feminist\n",
    "\n",
    "Topic  15\n",
    "cell, gene, dna, body, animal, molecule, genome, cancer, virus, genetic, specie, bacteria, technology, organism, biology, protein, tissue, environment, lab, evolution\n",
    "\n",
    "Topic  16\n",
    "company, money, dollar, business, market, cost, value, product, buy, pay, job, industry, decision, price, organization, financial, choice, spend, billion, billion dollar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis, pyLDAvis.sklearn\n",
    "from IPython.display import display\n",
    "\n",
    "# Setup to run in Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Create the visualization\n",
    "vis = pyLDAvis.sklearn.prepare(lda, nlp.transform(cleaned_document), nlp.vectorizer) #nlp.transform(cleaned_document) = X\n",
    "                                        #lda_cv_data?               \n",
    "# Export as a standalone HTML web page  \n",
    "# pyLDAvis.save_html(vis, 'lda.html')\n",
    "\n",
    "# Let's view it!\n",
    "display(vis)\n",
    "\n",
    "#(lda_mod(lda_rule), vect_data(fit_transf(data)), vect_mod(vectrule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_collection.find({\"transcript\": {\"$regex\": \"love\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transcripts_collection.find({\"transcript\": {\"$regex\": \"love\"}}).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
